#!/usr/bin/env python
"""Get light curves of supernova candidates from DES database.

If no SNIDs are given, all candidates with num_real >= 2 in the SNCAND
table are retrieved. Otherwise, only candidates with the given SNIDs
are retrieved (regardless of whether they have been scanned).

By default, the light curves are retrieved from the SNFORCE table, with
a query on SNCAND_ID. This behavior can be changed using the --table
flag. For the SNOBS table, the query is based on RA and DEC (within a
box of radius 1.08 arcsec). Note that the 'status' fields in the
SNFORCE and SNOBS tables have different meanings.

For the SNFORCE table, when there are multiple rows with the same
EXPNUM (exposure number from the EXPOSURE table), the row with the
latest image_id (from the IMAGE table) is returned; others are
discarded. This assumes that the latests image_id is the 'best'.
"""

import sys
import os
import glob
import math
from textwrap import dedent
from datetime import datetime
from optparse import OptionParser
import time
import json
try:
    from collections import OrderedDict as odict
except ImportError:
    odict = dict

import numpy as np
import cx_Oracle
import desdb
try:
    from desdb.desdb import PasswordGetter  # new location in desdb
except ImportError:
    from desdb import PasswordGetter  # old location in desdb

TABLES = ['snforce', 'snobs']
FORMATS = ['ascii', 'json', 'fits']
SUFFIXES = {'ascii': 'dat', 'json': 'json'}  # Single LC formats
SNOBS_BOXRAD = 1.08 / 3600.  # in degrees

# Utility ================================================================== #
def dict_to_array(d, keys=None):
    """Convert a dict of arrays (of equal length) to a structured array."""

    if keys is None:
        keys = d.keys()

    size = len(d[keys[0]])
    dtype = [(key, d[key].dtype) for key in keys]
    result = np.empty(size, dtype=dtype)
    for key in keys:
        result[key] = d[key]

    return result

# Writers ================================================================== #
def write_single_ascii(fname, data, meta, **kwargs):
    """Write a single light curve to an ASCII file"""

    delim = kwargs.get('delim', ' ')
    metachar = kwargs.get('metachar', '@')
    keys = data.dtype.names
    length = len(data)

    with open(fname, 'wb') as f:
        if meta is not None:
            for key, val in meta.iteritems():
                f.write('{0}{1}{2}{3}\n'.format(metachar, key, delim, val))

        f.write(delim.join(keys))
        f.write('\n')
        for i in range(length):
            f.write(delim.join([str(data[key][i]) for key in keys]))
            f.write('\n')

def write_single_json(fname, data, meta):
    """Write a single light curve to a JSON-format file"""

    # Build a dictionary of pure-python objects
    output = odict([('meta', meta), ('data', odict())])
    for key in data.dtype.names:
        output['data'][key] = data[key].tolist()

    with open(fname, 'wb') as f:
        json.dump(output, f, encoding=sys.getdefaultencoding())

def write_multiple_fits(meta_list, data_list, fname, header=None):
    """Write multiple light curves to a two-extension FITS file.

    Parameters
    ----------
    meta_list : list of dict
        All with same keys. Length must match that of ``data_list``.
    data_list : list of numpy.ndarrays
        All with same dtype.
    fname : str
        Output filename.
    header : dict
        Global header (added as FITS keywords in first extension).
    """

    from fitsio import FITS

    if len(data_list) != len(meta_list):
        raise ValueError("length of data_list and meta_list must match")

    # collect one giant array that will hold all of the data arrays.
    nrows = sum([len(d) for d in data_list])
    dtype = data_list[0].dtype
    alldata = np.empty(nrows, dtype)
    
    # Loop over data_list
    start = 0
    ind = 0
    while data_list:
        data = data_list.pop(0)  # remove first item
        end = start + len(data)
        alldata[start:end] = data
        meta_list[ind]['datastart'] = start
        meta_list[ind]['dataend'] = end
        start = end
        ind += 1

    meta_names = meta_list[0].keys()
    meta_arrays = [np.array([meta[n] for meta in meta_list])
                   for n in meta_names]
    allmeta = odict(zip(meta_names, meta_arrays))
    allmeta = dict_to_array(allmeta)

    f = FITS(fname,'rw')
    f.write(allmeta)
    f.write(alldata)
    f[0].write_keys(header)  # write global header
    f.close()

# Argument parser ========================================================== #
def parse_args():

    parser = OptionParser(usage='%prog [options] [SNID ...]',
                          description=__doc__)

    # Which candidates?
    parser.add_option("-n", type="int", default=-1,
                      help="maximum number of light curves to get (default "
                      "behaviour is to get all 'good' light curves)")
    parser.add_option("-a", "--all", default=False, action="store_true",
                      help="get all candidates, regardless of cand_type "
                      "(default: get only candidates that have "
                      "cand_type=0, indicating current, good)")
    parser.add_option("--infile", default=None,
                      help="Read SNIDs from this file instead of arguments.")
    parser.add_option("--mindate", default=None,
                      help="minimum candidate entry_date, format: YYYY-MM-DD")
    parser.add_option("--maxdate", default=None,
                      help="maximum candidate entry_date, format: YYYY-MM-DD")

    # Which fake candidates?
    parser.add_option("--fake", default=False, action='store_true',
                      help="get light curves for fake candidates (candidates "
                      "with snfake_id > 0) instead of real ones")
    parser.add_option("--minfakeid", default=-1, type="int",
                      help="minimum fake_id (only effective with --fake flag)")
    parser.add_option("--maxfakeid", default=-1, type="int",
                      help="maximum fake_id (only effective with --fake flag)")

    # Content: which table? optional metadata?
    parser.add_option("-t", "--table", default='snforce',
                      help=("table from which to retrieve photometry: "+
                            ",".join(TABLES) + " (default: snforce)"))
    parser.add_option("--galflux", default=False, action='store_true',
                      help="Include galflux in metadata (average of "
                      "SNOBS.GALFLUX column for nearby entries)")

    # Output formatting?
    parser.add_option("--format", default='ascii',
                      help=("file format for output: " + ",".join(FORMATS) +
                            " (default=ascii)"))
    parser.add_option("-b", "--bandnames", default='g,r,i,z',
                      help="comma-separated list of names of the DES "
                      "g, r, i, z bands, as they should appear in the "
                      "output files. example: 'desg,desr,desi,desz' default: "
                      "'g,r,i,z'")

    # Output location?
    parser.add_option("-o", "--output", default='.',
                      help="output directory (default='.')")
    parser.add_option("-c", "--clobber", default=False, action='store_true',
                      help="overwrite (refetch) existing files")

    # Verbosity?
    parser.add_option("-v", "--verbose", default=False, action='store_true',
                      help="print query string for all queries")

    opts, args = parser.parse_args(sys.argv[1:])

    if opts.infile is not None:
        with open(opts.infile, 'r') as f:
            snids = f.read().split()
    else:
        snids = args
    
    if opts.table not in TABLES:
        print "Table must be one of: " + ",".join(TABLES)
        sys.exit(1)

    opts.format = opts.format.lower()
    if opts.format == 'csv': opts.format = 'ascii'  # backwards compatibility
    if opts.format not in FORMATS:
        print "Format must be one of:", ', '.join(FORMATS)
        sys.exit(1)

    # Check output, depending on format
    if opts.format == 'fits':
        import fitsio   # fail immediately if fitsio not installed
        if not opts.output.endswith(".fits"):
            raise Exception("For fits format, output must have .fits suffix.")
        if os.path.exists(opts.output):
            raise Exception("Target output %s already exists" % opts.output)
    else:
        if not os.path.exists(opts.output):
            print "--output: must specify an existing directory"
            sys.exit(1)
        opts.output = opts.output.rstrip('/')

    # Turn bandnames into a lookup table
    names = opts.bandnames.split(',')
    if len(names) != 4:
        print "bandnames must have 4 comma-separated elements"
        sys.exit(1)
    opts.bandnames = {'g': names[0],'r': names[1],'i': names[2],'z': names[3]}

    return snids, opts

# Query string builders ==================================================== #
def build_cand_query(snids, opts):

    # Query for real candidates:
    if not opts.fake:
        q = dedent(
            """
            SELECT
                c.snid, c.ra, c.dec, c.cand_type, c.cand_desc, c.numepochs,
                c.numobs, c.num_real, c.num_artifact, c.num_unsure,
                c.num_unscanned, c.entry_date, c.snfake_id as fake_id
            FROM
                sncand c
            WHERE
                c.snfake_id=0
            """)
        if len(snids) == 0:
            q += "    AND c.num_real >= 2\n"

    # Query for fake candidates:
    else:
        q = dedent(
            """
            SELECT
                c.snid, c.ra, c.dec, c.cand_type, c.cand_desc, c.numepochs,
                c.numobs, c.num_real, c.num_artifact, c.num_unsure,
                c.num_unscanned, c.entry_date, c.snfake_id as fake_id,
                f.galid as fake_galid, f.z as fake_z,
                f.peakmjd as fake_peakmjd, f.host_angsep as fake_hostsep,
                f.hostmag_i as fake_hostmag_i
            FROM
                sncand c, snfake f
            WHERE
                c.snfake_id>0
                AND c.snfake_id=f.id
            """)
        if opts.minfakeid >= 0:
            q += "    AND c.snfake_id >= {0:d}".format(opts.minfakeid)
        if opts.maxfakeid >= 0:
            q += "    AND c.snfake_id <= {0:d}".format(opts.maxfakeid)

    # Get just certain candidates?
    if len(snids) > 0:
        q += "    AND c.snid in (" + ", ".join(snids) + ")\n"

    # specify cand_type=0 if we didn't request specific candidates:
    if len(snids) == 0 and (not opts.all):
        q += "    AND c.cand_type=0\n"

    # Any specific dates?
    if opts.mindate is not None:
        q += "    AND c.entry_date >= date '{0}'\n".format(opts.mindate)
    if opts.maxdate is not None:
        q += "    AND c.entry_date <= date '{0}'\n".format(opts.maxdate)

    # Maximum number of candidates to retrieve?
    if opts.n >= 0:
        q += "    AND rownum <= {0:d}\n".format(opts.n)

    q += "ORDER BY c.snid"
    return q


def build_snforce_query(snid, fake_id):

    q = """
        SELECT
            substr(e.object, 22, 2) field, i.ccd as ccdnum, e.band,
            e.mjd_obs, f.flux, f.flux_err, f.status, e.expnum,
            f.image_id, i.imagetype, i.run
        FROM
            exposure e, snforce f, image i
        WHERE
            f.sncand_id = {0:d}
            AND i.id = f.image_id
            AND e.id = i.exposureid
        ORDER BY
            e.expnum, f.image_id
        """.format(snid)

    # Get more information from the snobsinfo and snquality
    # tables.  This has to be implemented as a sub-query
    # because we need to `left outer join` to the snobsinfo
    # table based on *both* expnum and ccdnum and these values
    # come from two different tables.
    fullq_template = """
        SELECT
            ph.*,
            oi.psf_nea,
            oi.chip_sigsky,
            oi.chip_zero_point,
            oi.chip_zero_point_rms,
            nvl(q.snqual, 0) as snqual{0}
        FROM
            ({1}) ph
            LEFT OUTER JOIN snobsinfo oi
                ON oi.expnum = ph.expnum
                AND oi.tile = 0
                AND oi.ccdnum = ph.ccdnum
                AND oi.band = ph.band
                AND oi.run = ph.run
            LEFT OUTER JOIN snquality q
                ON ph.expnum = q.expnum{2}
        ORDER BY
            ph.expnum, ph.image_id
        """

    # If we're doing fakes, tack on some more info from FAKEIMG
    addfields = ""
    addjoin = ""
    if fake_id != 0:
        addfields = ", fi.truemag as fake_truemag"
        addjoin = """
            LEFT OUTER JOIN snfakeimg fi
                ON fi.expnum = ph.expnum
                AND fi.snfake_id = {0:d}""".format(fake_id)

    return fullq_template.format(addfields, q, addjoin)


def build_snobs_query(ra, dec):

    dra = SNOBS_BOXRAD * math.cos(dec * math.pi/180.)
    ddec = SNOBS_BOXRAD
    q = """
    SELECT
        substr(e.object, 22, 2) field, e.band, e.mjd_obs,
        o.flux, o.flux_err, o.mag, o.status, e.expnum
    FROM
        exposure e, snobs o
    WHERE
        o.ra between {0:f} and {1:f}
        AND o.dec between {2:f} and {3:f}
        AND o.exposureid=e.expnum
    ORDER BY
        e.expnum
    """.format(ra - dra, ra + dra, dec - ddec, dec + ddec)
    return q

def build_galflux_query(ra, dec):
    """Similar to above query, but just get average galflux for all
    entries within the SNOBS_BOXRAD"""

    dra = SNOBS_BOXRAD * math.cos(dec * math.pi/180.)
    ddec = SNOBS_BOXRAD
    q = """
    SELECT
        band, avg(galflux) as galflux
    FROM
        snobs
    WHERE
        ra between {0:f} and {1:f}
        AND dec between {2:f} and {3:f}
        AND band in ('g','r','i','z')  /* eliminate entries with band = 'N' */
    GROUP BY
        band
    """.format(ra - dra, ra + dra, dec - ddec, dec + ddec)
    return q


def build_sngals_query(snid):
    q = """
    SELECT
        g.photoz as host_photoz, g.photoz_err as host_photoz_err,
        g.specz as host_specz, g.specz_err as host_specz_err,
        g.specz_catalog as host_specz_catalog,
        g.separation as host_separation
    FROM
        sngals g
    WHERE
        g.snid = {0:d}
        AND g.host = 1
    """.format(snid)
    return q

# Main ===================================================================== #
if __name__ == "__main__":

    # Parse command-line args
    snids, opts = parse_args()

    # Get start time & database username.
    t0 = time.time()
    user = PasswordGetter().user

    # Create a connection to the database.
    conn = desdb.connect(dbname='desoper')

    # Build and run candidate query
    q = build_cand_query(snids, opts)
    if opts.verbose:
        print 78 * '=', '\n', "candidate query:\n", 78 * '=', '\n', q, '\n'
    cands = conn.quick(q)

    # specify order of keys in photometry tables
    photkeys = ['time', 'field', 'band', 'flux', 'fluxerr', 'zp', 'zpsys',
                'status', 'expnum']
    if opts.table == 'snforce':
        photkeys.extend(['image_id', 'ccdnum', 'psf_nea', 'chip_sigsky',
                         'chip_zero_point', 'chip_zero_point_rms', 'snqual'])
        if opts.fake:
            photkeys.append('fake_truemag')

    # FITS format: initialize containers to hold candidate data as we go.
    if opts.format == 'fits':
        meta_list = []
        data_list = []
        primaryheader = odict(
            [('dbuser', user),
             ('qrytime', datetime.now().strftime('%Y-%m-%dT%H:%M:%S')),
             ('qryutc', datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')),
             ('dbtable', opts.table)])

    # Loop over candidates
    print '',
    ncands = len(cands)
    nempty = 0
    for i, cand in enumerate(cands):

        # For single light curve formats, check if file already exists.
        if opts.format in SUFFIXES:
            fname = '{0}/des{1:08d}.{2}'.format(opts.output, cand['snid'],
                                                SUFFIXES[opts.format])
            if not opts.clobber and os.path.exists(fname):
                continue

        # Info line.
        print '\rsnid ={0:8d} ({1:6d}/{2:6d})'.format(cand['snid'],i+1,ncands),
        sys.stdout.flush()

        # Build the query string and run it
        if opts.table == 'snforce':
            q = build_snforce_query(cand['snid'], cand['fake_id'])
        elif opts.table == 'snobs':
            q = build_snobs_query(cand['ra'], cand['dec'])
        if opts.verbose:
            print '\n', 78 * '-', '\n', dedent(q)
        data = conn.quick(q, array=True)

        # Check if data is empty
        if len(data) == 0:
            nempty += 1
            continue

        # HACKS ----------------------------------------------------------
        # Get just the latest image ID for each exposure (expnum)
        # This counts on the data being ordered by (expnum, image_id).
        if opts.table == 'snforce':
            indicies_to_use = []
            for i in range(1, len(data)):
                if not (data['expnum'][i] == data['expnum'][i-1]):
                    indicies_to_use.append(i-1)
            indicies_to_use.append(len(data)-1)
            data = data[indicies_to_use]

            # Sometimes forced photometry was mistakenly run on
            # diff_single_diff imagetypes when a diff_nitecmb_diff
            # image was available. This was fixed in Fall 2013 (at
            # some point)
            #
            # TODO: remove this hack once snforce table is
            #       cleaned in off- season.  Mask rows where imagetype =
            #       'diff_single_diff' that are from deep fields or z band.
            mask = (
                (data['imagetype'] == 'diff_single_diff') &
                (np.char.endswith(data['field'], '3') | (data['band'] == 'z'))
                )
            data = data[~mask]
        # END HACKS -----------------------------------------------------

        # Convert `data` to a dictionary of arrays so we can manipulate
        # some of the columns.
        nrows = len(data)
        data = {key: data[key] for key in data.dtype.names}

        # Convert band names
        data['band'] = np.array([opts.bandnames[b.strip()]
                                 for b in data['band']])

        # Add zeropoint information
        if opts.table == 'snforce':
            data['zp'] = 31.4 * np.ones(nrows)
        elif opts.table == 'snobs':
            data['zp'] = data['mag'] + 2.5 * np.log10(data['flux'])
            del data['mag']
        data['zpsys'] = np.array(nrows * ['ab']) 

        # Rename some columns
        data['time'] = data.pop('mjd_obs')
        data['fluxerr'] = data.pop('flux_err')

        # Convert to a structured numpy array
        data = dict_to_array(data, keys=photkeys)

        # Create metadata.
        meta = odict()
        if opts.format != 'fits':
            meta['query_user'] = user
            meta['query_time'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')
            meta['query_utc'] = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')
            meta['query_table'] = opts.table
        meta.update(cand)

        # Convert entry_date from datetime object to a string.
        meta['entry_date'] = meta['entry_date'].strftime('%Y-%m-%dT%H:%M:%S')

        # Initialize host info in metadata.
        meta.update({'host_photoz': None, 'host_photoz_err': None,
                    'host_specz': None, 'host_specz_err': None,
                    'host_specz_catalog': None, 'host_separation': None})

        # Get host galaxy info from the SNGALS table.
        q = build_sngals_query(cand['snid'])
        if opts.verbose:
            print dedent(q)
        gals = conn.quick(q)
        if len(gals) == 1:
            meta.update(gals[0])
        elif len(gals) > 1:
            raise RuntimeError('More than one matching SNGALS entry with '
                               'HOST=1 for SNID=' + str(cand['snid']))

        # If requested, get average galflux from SNOBS table.
        if opts.galflux:
            meta.update({'galflux_g': None, 'galflux_r': None,
                         'galflux_i': None, 'galflux_z': None})
            q = build_galflux_query(cand['ra'], cand['dec'])
            if opts.verbose:
                print dedent(q)
            res = conn.quick(q)
            meta.update({'galflux_' + row['band']: row['galflux']
                         for row in res})

        # HACK ----------------------------------------------------------
        # `None` in num* columns means `0`.
        for key in meta:
            if key.startswith('num') and meta[key] is None:
                meta[key] = 0
        # ---------------------------------------------------------------

        if opts.format == 'ascii':
            for key in meta:  # Convert missing values to 'NULL'
                if meta[key] is None: meta[key] = 'NULL'
            write_single_ascii(fname, data, meta)

        elif opts.format == 'json':
            write_single_json(fname, data, meta)

        elif opts.format == 'fits':
            # Convert missing values to something writable.
            for key in meta:
                if meta[key] is None:
                    if key == 'host_specz_catalog':
                        meta[key] = ''
                    else:
                        meta[key] = np.nan
            meta_list.append(meta)
            data_list.append(data) 

    # for FITS format, write it all out.
    if opts.format == 'fits':
        write_multiple_fits(meta_list, data_list, opts.output,
                            header=primaryheader)

    elapsed = time.time() - t0
    print "\n{0:d} snids missing photometry were skipped".format(nempty)
    print "Total time: {0:d}m{1:05.2f}s".format(int(elapsed)/60, elapsed%60)
